---
title: 'Market Risk'
output: 
  flexdashboard::flex_dashboard:
  orientation: columns
  vertical_layout: fill
runtime: shiny
---

```{r}
# Header 
# author(s): "Nicholas Toscano"
# date: "March 16, 2018"

```
 
```{r, echo=FALSE, message=FALSE, warning=FALSE}
### R Environment Configuration
library(ggplot2)
library(shiny)
library(QRM)
library(qrmdata)
library(xts)
library(zoo)
library(psych)
library(knitr)
library(lubridate)
library(flexdashboard)
require (quantreg)
require(reshape2)
library(quadprog) 
library(matrixStats)
library(moments)
library(plotly) 
library(Quandl) # need to install package - not prev used
# rm(list = ls())

```

```{r, message=FALSE, warning=FALSE, include=FALSE}

nicks_api.key = 'JxJzaPDfqX-xftmzxfrz'

Quandl.api_key(nicks_api.key)
start_date = "2009-12-31"

#Historical daily pricing for LITECOIN close price
LITECOIN <- Quandl("BITFINEX/LTCUSD",  start_date=start_date)
 
#Historical daily pricing for Platinum:
PLATINUM <- Quandl("LPPM/PLAT",  start_date=start_date)
 
#Historical daily pricing for SILVER
SILVER <- Quandl("LBMA/SILVER",  start_date=start_date)
 
#BERKSHIRE - historical daily pricing of Petra BERKSHIRE company
BERKSHIRE <- Quandl("SSE/BRH",  start_date=start_date)
 
#3 month treasury
TBILL <- Quandl("USTREASURY/BILLRATES",  start_date=start_date)
 
USDZAR <- Quandl("FED/RXI_N_B_SF", start_date=start_date)

# EURO ZAR EXCHANGE RATES
EURZAR <-  Quandl("ECB/EURZAR",  start_date=start_date)

```


```{r, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
# Exploratory Data Analysis
# Output not displayed in dashboard - used to develop understanding data

### LITECOIN
str(LITECOIN)
# Note LITECOIN$Close and LITECOIN$Date required
# Date is already in date format
# 1647 records
range(LITECOIN$Date)
# "2013-09-08" "2018-03-12"
max(LITECOIN$Date) - min(LITECOIN$Date)
# 1646 Days, 1647 records - every day present
summary(factor(wday(LITECOIN$Date)))
# approx equal numbers of all seven days of week

### BERKSHIRE
str(BERKSHIRE)
# Note Date and Last are required fields
range(BERKSHIRE$Date)
# "2014-03-21" "2018-03-16"
max(BERKSHIRE$Date) - min(BERKSHIRE$Date)
# 1456 days range, 943 records. Looks like only 5 days a week
summary(factor(wday(BERKSHIRE$Date)))
# approx equal numbers for 5 days a week
# some days, particularly Mon are light - holidays

### SILVER
str(SILVER)
# Note Date and "EURO (PM)" are required fields
range(SILVER$Date)
# "2009-03-21" "2018-03-16"
max(SILVER$Date) - min(SILVER$Date)
# 3000 days range, 2076 records. Looks like only 5 days a week
summary(factor(wday(SILVER$Date)))
# approx equal numbers for 5 days a week
# some days, particularly Mon are light - holidays

### PLATINUM
str(PLATINUM)
# Note Date and "EUR (PM)" are required fields
range(PLATINUM$Date)
# "2009-12-31" "2018-03-15"
max(PLATINUM$Date) - min(PLATINUM$Date)
# 2996 days range, 2072 records. Looks like only 5 days a week
summary(factor(wday(PLATINUM$Date)))
# approx equal numbers for 5 days a week
# some days, particularly Mon are light - holidays

### EURZAR
str(EURZAR)
# Note Date and Value are required fields
range(EURZAR$Date)
# "2009-12-31" "2018-03-15"
max(EURZAR$Date) - min(EURZAR$Date)
# 2996 days range, 2103 records. Looks like only 5 days a week
summary(factor(wday(EURZAR$Date)))
# approx equal numbers for 5 days a week
# some days, particularly Mon are light - holidays

### EURZAR
str(USDZAR)
# Note Date and Value are required fields
range(USDZAR$Date)
# "2009-12-31" "2018-03-15"
max(USDZAR$Date) - min(USDZAR$Date)
# 2996 days range, 2052 records. Looks like only 5 days a week
summary(factor(wday(USDZAR$Date)))
# approx equal numbers for 5 days a week
# some days, particularly Mon are light - holidays

### TBILL
str(TBILL)
# Note Date and Value are required fields
range(TBILL$Date)
# "2010-01-01" "2018-02-01"
max(TBILL$Date) - min(TBILL$Date)
# 2997 days range, 2056 records. Looks like  1 record per week day
summary(factor(wday(TBILL$Date)))
# approx equal numbers for 5 days a week

```


```{r, message=FALSE, warning=FALSE, include=FALSE}
# Remove unnecessary columns before merging
LITECOIN2 <- LITECOIN[,c(1,5)]
BERKSHIRE2 <- BERKSHIRE[,c(1,4)]
SILVER2 <- SILVER[,c(1,7)]
PLATINUM2 <- PLATINUM[,c(1,6)]
TBILL <- TBILL[,c(1,4)] # 13 week bank discount rate

colnames(LITECOIN2) <- c("Date", "LITECOIN")
colnames(BERKSHIRE2) <- c("Date", "BERKSHIRE")
colnames(SILVER2) <- c("Date", "SILVER")
colnames(PLATINUM2) <- c("Date", "PLATINUM")
colnames(USDZAR) <- c("Date", "USDZAR")
colnames(EURZAR) <- c("Date", "EURZAR")
colnames(TBILL) <- c("Date", "TBILL")

# Tidy up
# rm(LITECOIN, BERKSHIRE, SILVER, PLATINUM, start_date)
```

```{r, message=FALSE, warning=FALSE, include=FALSE}
### Merge into a single data set
### Initial data set will have one row for each data present in any data set
### We will then trim out the records where we are missing data for some assets

asset_prices <- merge(LITECOIN2, BERKSHIRE2, by="Date", all.x=T, all.y=T)
asset_prices <- merge(asset_prices, SILVER2, by="Date", all.x=T, all.y=T)
asset_prices <- merge(asset_prices, PLATINUM2, by="Date", all.x=T, all.y=T)
asset_prices <- merge(asset_prices, USDZAR, by="Date", all.x=T, all.y=T)
asset_prices <- merge(asset_prices, EURZAR, by="Date", all.x=T, all.y=T)
asset_prices <- merge(asset_prices, TBILL, by="Date", all.x=T, all.y=T)

# Tidy up
rm(LITECOIN2, BERKSHIRE2, EURZAR, SILVER2, PLATINUM2)
#rm(USDZAR, TBILL)

### Remove rows with price gaps
asset_prices <- asset_prices[complete.cases(asset_prices),]
# reduces from 2622 rows to 891 records

```

```{r, eval=FALSE, include=FALSE}
# Analysis of resulting prices data set
str(asset_prices)
# one date, 7 numerics.  Col names are good
range(asset_prices$Date)
# "2014-03-21" "2018-03-09"
# many of the data sources had more data, but BERKSHIRE was limited
# leaves us with approx 4 years of data

max(asset_prices$Date) - min(asset_prices$Date)
# 1449 days range, 891 records. Looks like only 5 days a week
summary(factor(wday(asset_prices$Date)))
# approx equal numbers for 5 days a week
# some days, particularly Mon are light - holidays

### Check ranges and look up FX rates on internet
range(asset_prices$USDZAR)
#10.2990 16.8845
range(asset_prices$EURZAR)
#12.7006 18.2896
### Note, in each case, this is the number of ZAR for 1 unit of USD or EUR
```



```{r}
# Adjust all prices into ZAR
### From Quandl Doco
### LITECOIN is in ZAR - no FX needed
### PLATINUM we used EURO - convert using EUR FX
### SILVER we used EURO - convert using EUR FX
### BERKSHIRE is on Stuttgart - quoted in EURO - convert using EUR FX
### TBILL is in USD - convert using USD FX

asset_prices$LITECOIN <- asset_prices$LITECOIN * asset_prices$EURZAR
asset_prices$PLATINUM <- asset_prices$PLATINUM * asset_prices$EURZAR
asset_prices$SILVER <- asset_prices$SILVER * asset_prices$EURZAR
asset_prices$BERKSHIRE <- asset_prices$BERKSHIRE * asset_prices$EURZAR


# remember TBILL is not a price, it's an annual rate
TBILL.returns <- asset_prices[-1,8]/365 # create vector of rates
asset_prices <- asset_prices[,-c(8)] # remove from prices data frame

```



```{r}

### Create returns

LITECOIN.returns <- diff(log(asset_prices$LITECOIN)) * 100
BERKSHIRE.returns <- diff(log(asset_prices$BERKSHIRE)) * 100
SILVER.returns <- diff(log(asset_prices$SILVER)) * 100
PLATINUM.returns <- diff(log(asset_prices$PLATINUM)) * 100

USDZAR.returns <- diff(log(asset_prices$USDZAR)) * 100

TBILL_ZAR.returns <- ((1.0 + (TBILL.returns/100))  * (1.0 + (USDZAR.returns)/100)) - 1.0

asset_returns <- data.frame(asset_prices[-1,1]  , 
                            TBILL_ZAR.returns , 
                            LITECOIN.returns,
                            BERKSHIRE.returns,
                            SILVER.returns,
                            PLATINUM.returns,
                            TBILL.returns)

colnames(asset_returns) <- c("Date", "TBILL_ZAR", "LITECOIN", "BERKSHIRE", "SILVER", "PLATINUM", "TBILL")

# prep for time series - convert index to dates
rownames(asset_returns) <- asset_returns$Date

# Tidy up
rm(LITECOIN.returns, BERKSHIRE.returns, SILVER.returns, PLATINUM.returns, USDZAR.returns, TBILL_ZAR.returns, TBILL.returns)

### Create XTS Time Series of Returns
asset_returns.xts <- as.xts(asset_returns[,-1])

## absolute returns and signs
asset_returns_abs <- asset_returns
asset_returns_abs[, 2:6] <- abs(asset_returns_abs[, 2:6])
asset_returns_abs.xts <- abs(asset_returns.xts)

asset_returns_sign <-  ifelse(asset_returns > 0, 1, ifelse(asset_returns < 0, -1, 0))
asset_returns_sign.xts <- as.xts(asset_returns_sign)


```



```{r}
# flexdashboard layout begins here
```

Preparation   
===================================== 

Column: Data Assessment  {.tabset}  
--------------------------------------------------------------------------

#### Business Problem   

A prominent South African manufacturer of fine jewelry has built up a substantial capital reserve of ZAR 5.9 Bn.  The company's P&L is stated in South African rand (ZAR).   The company has built this reserve despite facing high volatility in the prices of the raw materials it consumes to make the jewelry (SILVER, BERKSHIRE and platinum).  The company has also recently been pressured by one of the major customers to change business terms so that all future payments will be in LITECOIN.  As a result, the CFO expects that the company will continually have a long position in LITECOIN and a consequential risk of devaluation of LITECOIN affecting its P&L.

The CFO has asked the team to construct a hedging portfolio that protects against supply chain price increases by holding long positions in SILVER, BERKSHIRE and platinum; and which hedges against holding LITECOINs.

The CFO advised that the following be included in your analysis:  
* The portfolio should be optimized using a risk free rate equivalent to the US 3-month T-Bill yield as published on 2/28/2018.  The rate used should not include any currency effects converting income to ZAR.  
* Where possible commodity prices should be used to model the hypothetical historical portfolio, but if they are not available then the price of suitable equity can be used as a substitute.  You can use either a single security price for each commodity, or a blend of multiple securities representing that commodity.  

#### Business Questions   

The CFO has asked the team to specifically answer the following questions:
How volatile is each of the securities?  
* Are there signs in the historical returns of the securities that indicate that the portfolio lacks diversification?  
* What losses might we expect from the portfolio on the worst 5% of days   
* If we model the portfolio's losses by fitting a Pareto distribution, what losses do we observe from the fitted distribution?    
* With just the four securities, each susceptible to market risk, what is the optimal mix for our ZAR 6B portfolio, and what return will this provide? (Neil, optimization)  
* If we were to introduce a risk-free security into the portfolio to reduce the volatility risk, how much does the risk reduce for a range of target returns?   

#### Portfolio Variation   

The CFO also asked the team do some sensitivity analysis on the portfolio.  Team members should switch out one of the securities and adjust the analysis as follows:  
* Replace LITECOIN with another crypto currency (Nick)
* Replace SILVER with silver (Jason)
* Replace platinum with palladium (Karen)


### Commodity Prices    

```{r}
ggplot(data=asset_prices, aes(x=Date)) +
  theme_light() +
  geom_line(aes(y=LITECOIN), color="thistle") +
  geom_line(aes(y=SILVER), color="orange") +
  geom_line(aes(y=BERKSHIRE*1000), color="lightblue") +
  geom_line(aes(y=PLATINUM), color="darkgrey") +
  ggtitle("Commodity Price History")+
  labs(x= "Date", y="Price") +
  annotate("text", x = as.Date("2018-01-01") , y = 200000, label = "LITECOIN", color="thistle") +
  annotate("text", x = as.Date("2018-01-01")  , y = 30000, label = "SILVER", color="orange") +
  annotate("text", x = as.Date("2015-01-01") , y = 50000, label ="BERKSHIRE (1000 shares)", color="lightblue" ) +
  annotate("text", x = as.Date("2017-07-01") , y = 2000, label ="PLATINUM", color="darkgrey" )
  
```

### Returns   

```{r echo=FALSE, results='HIDE',message=TRUE}
# Reshape the time series data by unpivoting so that the column names are a field

tmp <- melt(data = asset_returns, 
            id = "Date", # date column 
            measure.vars = c(2:6)) # the five returns columns

# Create plots

ggplot(data = tmp, aes(x=Date, y=value, colour=variable)) +
  theme_light() +
  geom_line() + 
  scale_color_manual(name  ="Variable",values=c("green", "thistle",  "lightblue", "orange", "dark grey"),
            labels=c("Return TBill", "Return LITECOIN", "Return Diamond", "Return SILVER", "Return Platinum")) +
  facet_wrap(~variable, ncol=1 )+ ggtitle("Summary of Returns") +
  labs(x= "Date", y="Value")

rm(tmp)

```

### Size and Direction   

```{r echo=FALSE, results='HIDE',message=TRUE}
# Reshape the time series data by unpivoting so that the column names are a field

tmp <- melt(data = asset_returns_abs,
            id = "Date", # date column 
            measure.vars = c(2:6)) # the three returns columns
# Create plots

ggplot(data = tmp, aes(x=Date, y=value, colour=variable))+
  theme_light() +
  geom_line() + 
  scale_color_manual(name  ="Variable",values=c("green", "thistle",  "lightblue", "orange", "dark grey"),
            labels=c("Return TBill", "Return LITECOIN", "Return Diamond", "Return SILVER", "Return Platinum")) +
  facet_wrap(~variable, ncol=1 )+ 
  ggtitle("Commodity") +
           xlab("Adjusted Price") + ylab("Observations")

```

Column: Discussion   {data-width=350}
--------------------------------------------------------------------------

### Discussion  

In the prices plot to the left, we observe that the prices of the metals, particularly Nickel display volatility as evidenced by intermittent period of see-sawing price changes.

The baseline data set was daily prices of three commodities: Nickel, Aluminum and Copper.  Here we can see the dates range from `r min(asset_prices$DATE)` to `r max(asset_prices$DATE)`, and there are `r nrow(asset_prices)` observations included in the data.  Summarizing by weekday, we can see here that there are approximately 5 years of daily data, almost evenly distributed across weekdays.  There is no data for weekends, and we hypothesize that there is data missing on public holidays (in some undefined market).

After calculating daily returns using the log of the difference of daily prices, R's summary() command shows us the statistical summary of the daily returns of each metal.  All values are percentages.

```{r}
#format(as.Date("2015-01-07", format = "%Y-%d-%m"))

daycounts <- factor(wday(asset_prices$Date))

levels(daycounts) <- c("Mon", "Tue", "Wed", "Thu", "Fri")

kable(t(as.matrix(summary(daycounts))), caption="Distribution of Data By Weekday") 

```


```{r}
# display summary
summary(asset_prices)
```

In the returns plots on the left, we see that volatility is indicated by returns swinging between positive and negative extremes on successive days.  We also see the volatility occurring in the absolute returns as blocks of higher peaks.


Correlation  
====================================

```{r}
# Compute log differences percent
# using as.matrix to force numeric
# type
data.r <- diff(log(as.matrix(asset_prices[, -1]))) * 100
# Create size and direction
size <- na.omit(abs(data.r)) # size is indicator of volatility

# head(size)
colnames(size) <- paste(colnames(size),".size", sep = "") # Teetor
direction <- ifelse(data.r > 0, 1, ifelse(data.r < 0, -1, 0)) # another indicator of volatility

colnames(direction) <- paste(colnames(direction),".dir", sep = "")

# Convert into a time series object:
# 1. Split into date and rates
dates <- as.Date(asset_prices$Date[-1], "%m/%d/%Y")
dates.chr <- as.character(asset_prices$Date[-1])
# str(dates.chr)

values <- cbind(data.r, size, direction)

data.df <- data.frame(dates = dates,
                      returns = data.r, 
                      size = size, 
                      direction = direction)

data.df.nd <- data.frame(dates = dates.chr,
                         returns = data.r, 
                         size = size, 
                         direction = direction,
                         stringsAsFactors = FALSE)

# non-coerced dates for subsetting on
# non-date columns 2. Make an xts
# object with row names equal to the
# dates

data.xts <- na.omit(as.xts(values, dates)) #order.by=as.Date(dates, '%d/%m/%Y')))

data.xts2 <- subset( data.xts, select = -c(USDZAR, EURZAR,USDZAR.size, EURZAR.size, USDZAR.dir, EURZAR.dir) )

data.zr <- as.zooreg(data.xts2)
returns <- data.xts2
# Market analysis of the stylized
# facts and market risk preliminaries

tmp <- melt(data = data.df[, c(1,5,6,7)],
id = "dates", # date column 
measure.vars = c(2:4)) # the three returns columns

```


Column: Overview  {data-width=350}
--------------------------------------------------------------------------

### Overview

To compare our metal commodities to each other, we chose to use correlation and regression techniques, as well as the data moments function. Correlation and regression helps us to understand the degree to which the metal prices affect each other or have a common response to economic events. Through these techniques, we can see these relationships both on a particular day and with a delayed effect. Data moments is also a useful tool in understanding the base statistics found in each variable.  

When developing a portfolio of metals, it is important to understand how the metals interact with each other to ensure proper diversification.  

Column: Charts  {.tabset}
--------------------------------------------------------------------------

```{r, message=FALSE, warning=FALSE, include=FALSE}
# FUNCTION DEFINITION
# corr.rolling()
# INPUT: x - an xts object with two or more series
# OUTPUT: corr.r - an xts object containing the correlations between the series
# TRANSFORMATIONS: - calculate the correlation coefficients of each time series

corr.rolling <- function(x){
  dim <- ncol(x)
  corr.r <- cor(x)[lower.tri(diag(dim),
                             diag = FALSE)]
  return(corr.r)
}

```


```{r, message=FALSE, warning=FALSE, include=FALSE}
ALL.r <- na.omit(data.xts2[, 1:4])# Only five series here

window <- 90 #define the number of observations in each correlation calc

corr.returns <- rollapply(ALL.r, width = window,
                          corr.rolling, 
                          align = "right", 
                          by.column = FALSE)

#colnames(ALL.r)
#colnames(corr.returns)

# define column names
colnames(corr.returns) <- c('BTC.BERKSHIRE','BTC.SILVER', 'BTC.PLATINUM', 'BERKSHIRE.SILVER','BERKSHIRE.PLATINUM', 'SILVER.PLATINUM')

# create a data frame of output
corr.returns.df <- data.frame(Date = index(corr.returns),
                              LITECOIN.BERKSHIRE = corr.returns[, 1], 
                              LITECOIN.SILVER = corr.returns[, 2],
                              LITECOIN.PLATINUM = corr.returns[, 3],
                              BERKSHIRE.SILVER = corr.returns[, 4],
                              BERKSHIRE.PLATINUM = corr.returns[, 5],
                              SILVER.PLATINUM = corr.returns[, 6])
```


```{r, message=FALSE, warning=FALSE, include=FALSE}
# Market dependencies
library(matrixStats)

R.corr <- apply.monthly(as.xts(ALL.r), FUN = cor)   #Calculate monthly summary of correlation

R.vols <- apply.monthly(ALL.r, FUN = colSds) # from MatrixStats\t

# Form correlation matrix for one month
R.corr.1 <- matrix(R.corr[20, ], nrow = 4, ncol = 4, byrow = FALSE)

# set row and column names
rownames(R.corr.1) <- colnames(ALL.r[,1:4])
colnames(R.corr.1) <- rownames(R.corr.1)

```

```{r, message=FALSE, warning=FALSE, include=FALSE}
# create a time series of just the correlations
R.corr <- R.corr[, c(2, 3, 4, 7, 8, 12)]
colnames(R.corr) <- colnames(corr.returns)

colnames(R.vols) <- c("LITECOIN.vols", "BERKSHIRE.vols", "SILVER.vols", "PLATINUM.vols")  

#correlation how you move with other and stdv how you move from your mean.
R.corr.vols <- na.omit(merge(R.corr,R.vols))
LITECOIN.vols <- as.numeric(R.corr.vols[, "LITECOIN.vols"])
BERKSHIRE.vols <- as.numeric(R.corr.vols[, "BERKSHIRE.vols"])
SILVER.vols <- as.numeric(R.corr.vols[, "SILVER.vols"])
PLATINUM.vols <- as.numeric(R.corr.vols[, "PLATINUM.vols"])


# hist(rho.fisher[, 1])
LITECOIN.corrs <- R.corr.vols[, 1]
# hist(nickel.corrs)
# class(nickel.corrs)

```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# FUNCTION 
# INPUTS:
# one, two - a pair of vectors representing time series
# main - a title for use on a chart
# lag - a correlation lag parameter
# color - a color to use for a plot
# OUTPUTS - a cross correlation function (ccf) plot

run_ccf <- function(one, two, main = title.chg, lag = 20, color = "red") {
      # one and two are equal length series
      # main is title lag is number of lags
      # in cross-correlation color is color
      # of dashed confidence interval
      # bounds
      stopifnot(length(one) == length(two)) # error out if diff length TS
      one <- ts(one) # convert to time series
      two <- ts(two) # convert to time series
      main <- main
      lag <- lag
      color <- color
      # now run the ccf plot
      ccf(one, 
          two, 
          main = main, 
          lag.max = lag,
          xlab = "", 
          ylab = "", 
          ci.col = color)
      # end run_ccf
      }
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
## Load the data_moments() function
## data_moments function 
## INPUTS: r
## vector OUTPUTS: list of scalars
## (mean, sd, median, skewness,
## kurtosis)
data_moments <- function(data) {
  library(moments) 
  data <- as.matrix(data)
  mean.r <- colMeans(data)
  median.r <- colMedians(data)
  sd.r <- colSds(data)
  IQR.r <- colIQRs(data)
  skewness.r <- moments::skewness(data) 
  kurtosis.r <- moments::kurtosis(data)
  
  result <- data.frame(mean = mean.r,
                       median = median.r, 
                       std_dev = sd.r,
                       IQR = IQR.r, 
                       skewness = skewness.r,
                       kurtosis = kurtosis.r)
  return(result) 
  }
```

### Statistical Summary  

First, we employed the data_moments() function to present the summary statistics for our signed returns and absolute returns distributions. This provided a base understanding of each data variable.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
#Run data_moments()
# Run data_moments()
answer <- data_moments(data.xts[,1:4]) 
answer <- round(answer, 4) 
knitr::kable(answer, digits = 3) # Build pretty table

```
### Auto and Cross Correlations 

####

We then began our analysis into the internal and cross relationships the commodities had with each other by using the acf() autocorrelation and ccf() cross correlation functions on the returns and size. Auto and cross correlations functions show us the correlation coefficient of the time series for the same date (lag 0) and for a set of days lag. The lag essentially asks, does one time series predict another time series x days in the future? 

Lagged correlations are valuable as their predictive power can be used to make speculative investments.

####

```{r, echo=FALSE, message=FALSE, warning=FALSE}

acf(coredata(data.xts[, 1:3])) # returns
acf(coredata(data.xts[, 4:6])) # sizes

# run first to look at CCF of signed returns

title <- "LITECOIN - BERKSHIRE Returns"
one <- data.zr[, 1]
two <- data.zr[, 2]
run_ccf(one, 
        two, 
        main = title, 
        lag = 20,
        color = "red")

# now for volatility (sizes of returns)

one <- abs(data.zr[, 1])
two <- abs(data.zr[, 2])
title <- "LITECOIN - BERKSHIRE: Volatility"
run_ccf(one, 
        two, 
        main = title, 
        lag = 20,
        color = "red")

```

####

In review of these charts, we identified:

* On the diagonal of auto-correlations, the only significant marks are on zero lag. This indicates that one day's return does not predict subsequent days returns for any metal. The only exception to this is aluminum, whose return yesterday is mildly predictive that today's return will be of the opposite sign. By this past trend, we can determine that the next time the price goes down for aluminum, we can expect high returns the next day. 

* Copper and nickel have some positive relationships at zero lag (they tend to behave the same way on the same day). We can predict nickel price by lagging the copper price, i.e. the nickel price 4 days ago and 6 days ago is also predictive of the copper price today. The other lags are at best marginal.  Copper can predict nickel and nickel can predict the copper, representing cross correlation. 

* Nickel and aluminum have a modest same-day, same-direction correlation. 

We observe similar patterns in the fourth chart of correlations of absolute returns. The exception here is aluminum, where mutliple successive prior days are predictive of the current day.  The difference between aluminum on the signed and absolute returns suggests that aluminum return volatility occurs in clusters, with large days being predictive of future large days, and small days being predictive of future small days.

### Monthly Correlations and Volatility  

####

We then began to analyze the history of the correlations for the metal exchange rates by using the rolling correlation and rollapply functions. The rolling correlation function receives an xts time series object with multiple series and calculates the correlation between each of the series. 

In order to do this efficiently, it performs an outer product of the time series and its inverse using R's 'cor' Pearson correlation function. It then takes the lower triangle (i.e. the cells below the diagonal) to the resulting matrix. The results are returned as a vector of the correlations. 

The rollapply function is used to step through the rows in a time series. On each step it takes the current observation and its n preceding items and calls the function to be performed on them. The results are assembled into a time series where the index of the time series is the index of the nth row in the input time series.

In this case:  

* We pass the first three columns of our data time series.
* We call our corr_rolling function to calculate the correlation coefficients for the data in each step.
* We selected n=90 as this is a reasonable number to use for a Pearson correlation
* We do not expect a correlation calculation for the first 89 days because the function will not operate until there are sufficient observations accumulated.

With this configuration, we expect the rollapply function to build a time series whose first observation is 90 days after the beginning of the metal rate xts time series, where each column in the time series represents the trailing 90 days correlation coefficients of a pair of metal rates.

Below is a graph to represent the results:

```{r, echo=FALSE, message=FALSE, warning=FALSE} 

#Create column headers and plot to represent relationships

colnames(corr.returns) <- c("BTC.BERKSHIRE","BTC.SILVER", "BTC.PLATINUM", "BERKSHIRE.SILVER","BERKSHIRE.PLATINUM", "SILVER.PLATINUM") 

plot.xts(corr.returns, 
         multi.panel = 6,
         grid.ticks.on = FALSE,
         minor.ticks = NULL,
         yaxis.right = FALSE,
         grid.col = "azure2", 
         main="Trailing 90 Day Correlations", 
         xlab="",
         cex = 0.5,
         col=c("thistle", "aquamarine3", "darkseagreen2", "chocolate3", "cyan4", "darkSILVERenrod3")
         )
```

#### 

Once we've determined the rolling correlation, we then used apply.monthly to summarize the results by month. We also used the cor function to calculate the Pearson correlation coefficient between the different exchange rate returns.

In order to see one month's worth of data, we created a matrix and selected one row. Below is a representation of the 20th set of correlations, which correspond to the month of August 2013.

```{r}

# display formatted table
knitr::kable(R.corr.1, digits=4)

```

####

Once the correlations were determined and summarized monthly, we then proceeded to calculate the monthly volatility for the returns in our time series. Again we use the apply.monthly function and couple this with the "Column standard deviations" function colSds. Standard deviation is used here as a measure of volatility because it summarizes the degree to which the observations digress from the mean. More deviation in the values of the returns (i.e. returns much higher or lower than the mean), implies more volatility.

Once we calculated both the monthly correlations and volatility, we then merged them into one times series using the merge function. Because each correlation matrix consists of a diagonal of 1's, and two triangles with identical information, we needed to select the columns that represented only one of the two triangles (we chose the upper triangle).

Below is a graph to represent our results of having merged correlations and volatility together: 

```{r, echo=FALSE, message=FALSE, warning=FALSE}

plot.xts(R.corr.vols, 
         multi.panel = 4,
         grid.ticks.on = FALSE,
         minor.ticks = NULL,
         yaxis.right = FALSE,
         grid.col = "azure2",
         main="Metal Market - Correlation & Volatility", 
         xlab="",
         cex = 0.5,
         col=c("thistle", "aquamarine3", "darkseagreen2", "chocolate3", "cyan4", "darkSILVERenrod3", "dodgerblue2", "darkslategray3", "darksalmon", "darkorchid1")
         )

```

### Regression  

#### 

Having reviewed the correlations, we then created a regression model and selected the nickel and copper data to investigate further.

The regression is of the format $y = ax$ where $a$ is the slope coefficient calculated by the regression. This summary plot displays the optimum regression coefficient for each decile (on the Y axis), with the deciles of data points on the X axis. It then links the points as a black dashed line, with a grey confidence interval showing the spread of possible correlation coefficients for each decile of data points.  It overlays the correlation coefficients for the 0.05, 0.5 and 0.95 quantiles as horizontal red lines.  The red middle line is our quantile regression model coefficient for the 0.5 (median) quantile.  The red dotted lines are confidence level for correlation - the 0.05 and 0.95 quantiles.  

Below is a graph of the quantile regression:

#### 

```{r, echo=FALSE, message=FALSE, warning=FALSE}
taus <- seq(0.05, 0.95, 0.05) # Roger Koenker UI Bob Hogg and Allen Craig
fit.rq.LITECOIN.SILVER <- rq(LITECOIN.corrs ~ SILVER.vols, tau = taus)
fit.lm.LITECOIN.SILVER <- lm(LITECOIN.corrs ~ SILVER.vols)

plot(summary(fit.rq.LITECOIN.SILVER), 
     parm = "SILVER.vols",
     main = "LITECOIN - SILVER Correlation Sensitivity to LITECOIN Volatility")
```

####

We can interpret this plot by asking whether the calculated coefficients for each decile fall within the red confidence interval. In an ideal model the black data points would run along the solid red line, and the grey bounds would align with the red dotted lines.  This model falls far short of that ideal. For this regression to be reliable, the black data points would all be within the red dotted lines. 

We can see for the central and higher quantiles that the coefficents are within the red dotted lines of the confidence interval for the regression coefficient. For the lower quantiles the coefficients are outside the confidence interval. In practical terms, this means that regression is not a good way to model the relationship between correlation and volatility, because a regression model could not simultaneously explain the relationship for the higher quantiles (ie most positive returns) as well as the lower quantiles (ie the most negative returns).

One way to think of this is that the relationship between correlation and volatility is non-linear. We may be able to develop one linear model to represent the lower quantiles (0.0-0.4), and another linear model to represent the higher quantiles (0.5-1.0).





Value at Risk  
====================================

Column: Value at Risk    {data-width=350}
--------------------------------------------------------------------------

### Overview    

Value at Risk (VaR) is a technique that identifies the most extrement negative events that might harm our business so that mitigating steps (eg insurance) can be taken to protect our profit from such events.  With VaR we study the distribution of market events, and agree a threshold, beyond which we may wish to protect from events.

We can use R's quantiles function to identify the return value associated with our threshold.

The concept of Expected Shortfall (ES) works hand-in-hand with VaR, in that it is defined as the mean of the events beyond the threshold.  One way to think of it is that, if VaR defines a very bad day, ES is how bad we should expect (on average) very bad days to be.

We should discuss here what our VaR threshold should be.  Because our company will hold physical commodities while they are shipped, we have a long exposure to the commodities (ie we own them).  We are therefore liable to negative returns - if there is a negative return, the price falls, and we sell our metals for less than we bought them.

Our quantiles are laid out in increasing order of size, so the smallest (ie most negative) returns are in the lowest quantiles.  We therefore need to look at VaR as being a very low quantile (we choose 0.05), and consider ES as the mean of the values below the 0.05 quantile.


Column: VaR Threshold  {.tabset} 
--------------------------------------------------------------------------

### Value at Risk  

####  

```{r, echo=FALSE, message=FALSE, warning=FALSE}

# Data frame for Commodity 1 with Return and Historical Distribution labels  

returns1 <- asset_returns.xts[, 2]
colnames(returns1) <- "Returns" #kluge to coerce column name for df

returns1.df <- data.frame(Returns = returns1[,1], 
                          Distribution = rep("Historical",each = length(returns1)))

# VaR threshold 

alpha <- 0.95 # reactive({ifelse(input$alpha.q>1,0.99,ifelse(input$alpha.q<0,0.001,input$alpha.q))})

# Value at Risk for Commodity 1

VaR.hist <- quantile(asset_returns.xts[,2], 1-alpha) 
VaR.text <- paste("Value at Risk =", round(VaR.hist, 2))

# Determine the Max y value for the density plot. This will be used to place the text above the plot

VaR.y <- max(density(returns1.df$Returns)$y)

# Value at Risk for Comodities 2-4

VaR.hist2 <- quantile(asset_returns.xts[,3], 1-alpha) 
VaR.hist3 <- quantile(asset_returns.xts[,4], 1-alpha) 
VaR.hist4 <- quantile(asset_returns.xts[,5], 1-alpha) 


# Expected Shortfall for Commodity 1

ES.hist <- mean(returns1[returns1 < VaR.hist]) 
ES.text <- paste("Expected Shortfall =", 
                 round(ES.hist, 2))

# Expected Shortfall for Commodities 3-5

returns2 <- asset_returns.xts[, 3]
returns3 <- asset_returns.xts[, 4]
returns4 <- asset_returns.xts[, 5]

ES.hist2 <- mean(returns2[returns2 < VaR.hist2])
ES.hist3 <- mean(returns3[returns3 < VaR.hist3])
ES.hist4 <- mean(returns4[returns4 < VaR.hist4])

p4 <- ggplot(returns1.df, 
            aes(x = Returns, fill = Distribution)) + 
    theme_light() +
  geom_density(alpha = 0.5) + 
  geom_vline(aes(xintercept = VaR.hist), 
             linetype = "dashed", 
             size = 0.1,
             color = "firebrick1") + 
  geom_vline(aes(xintercept = ES.hist), 
             size = 0.1, 
             color = "firebrick1") +
  annotate("text", x = 2 + VaR.hist, y = VaR.y * 1.05, label = VaR.text) +
  annotate("text", x = 1.5 + ES.hist, y = VaR.y * 1.1, label = ES.text) +
  scale_fill_manual(values = "dodgerblue4")+
  labs(x= "Returns", y="Density")

ggplotly(p4)

```

####

The plot above shows the distribution of daily returns for LITECOIN. The dashed line is the  Value at Risk marker, the solid line is our Expected Shortfall marker. These tell us that with 95% confidence, we will not lose more than 7.89%.  For the remaining 5% of days, our average loss is expected at -13.95%. 

For comparison purposes, BERKSHIRE has an VAR of `r -quantile(asset_returns.xts[,3], 1-alpha)` and ES of `r -mean(returns2[returns2 < VaR.hist2])`; SILVER has a VAR of `r -quantile(asset_returns.xts[,4], 1-alpha)` and ES of `r -mean(returns3[returns3 < VaR.hist3])`; and Platinum has a SILVER has a VAR of `r -quantile(asset_returns.xts[,5], 1-alpha)` and ES of `r -mean(returns3[returns3 < VaR.hist4])`

These Value at Risk and Expected Shortfalls tell us that LITECOIN is by far the commodity with the most value at risk, with BERKSHIRE being the second in risk.  

### Portfolio VaR 

####  

```{r, echo=FALSE, message=FALSE, warning=FALSE}

# Now for Loss Analysis - Get last prices

price.last <- as.numeric(tail(asset_prices[2:5], n = 1))

# Specify the positions

position.rf <- c(1/4, 1/4, 1/4, 1/4)

# Compute the position weights

w <- position.rf * price.last

# Fan these the length and breadth of the risk factor series

weights.rf <- matrix(w, 
                     nrow = nrow(asset_returns[2:5]),
                     ncol = ncol(asset_returns[2:5]), 
                     byrow = TRUE)

# head(rowSums((exp(asset_returns/100)-1)*weights.rf), n=3) 
# We need to compute exp(x) - 1 for very small x: expm1 accomplishes this
# head(rowSums((exp(asset_returns/100)-1)*weights.rf), n=4)

loss.rf <- -rowSums(expm1(asset_returns[2:5]/100) * weights.rf)
loss.rf.df <- data.frame(Loss = loss.rf, Distribution = rep("Historical", each = length(loss.rf)))

```

####  

We then extend this idea to construct a portfolio of different commodities. In the first instance, we construct a portfolio of positions, each position being one fourth of the price of our commodities based on the most recent prices. This is somewhat like a Market Capitalization weighted stock index. The total value of this portfolio is calculated as \$ `r format(sum(weights.rf[1,]), big.mark=",", scientific=FALSE, digits=2)`.  

We are interested in understanding the performance of a portfolio of \$250Bn, so we can later extrapolate by multiplying results by $250M/31,743.

We then created a time series of the returns for this portfolio based on the historical returns of each commodity's impact. In our portfolio construction, we introduced a negative multiplier for our returns, so that the most harmful returns are the most positive as indicated by the largest quantiles. We then select a 0.95 VaR threshold and consider ES as the mean of the items beyond that threshold. With our portfolio constructed, we performed our VaR analysis.

####

```{r, echo=FALSE, message=FALSE, warning=FALSE}

## Simple Value at Risk and Expected Shortfall

alpha.tolerance <- 0.95

VaR.hist <- quantile(loss.rf, probs = alpha.tolerance, names = FALSE)
ES.hist <- mean(loss.rf[loss.rf > VaR.hist])

VaR.text <- paste("Value at Risk =\n", round(VaR.hist, 2)) # ='VaR'&c12
ES.text <- paste("Expected Shortfall \n=", round(ES.hist, 2))
title.text <- paste(round(alpha.tolerance * 100, 0), "% Loss Limits")

# Use histogram bars instead of the smooth density

p3 <- ggplot(loss.rf.df, 
       aes(x = Loss, fill = Distribution)) + 
  theme_light() +
  geom_histogram(alpha = 0.8) +
  geom_vline(aes(xintercept = VaR.hist), 
             linetype = "dashed", 
             size = 0.1,
             color = "firebrick1") + 
  annotate("text", x = VaR.hist, y = 40, label = VaR.text) +
  #geom_vline(aes(xintercept = ES.hist), 
  #           size = 0.1, 
  #           color = "firebrick1") +
  annotate("text", x = ES.hist, y = 20, label = ES.text) + xlim(0, 500) +
  labs(x= "Loss", y="Count", title=title.text) +
  theme(legend.position='none')

# Note - second vline was excluded because ggplotly causes flexdashboard to fail to render when it is included

ggplotly(p3)

```

####

The red areas are our distribution of returns. The dashed line is the Value at Risk marker. This is where we are 95% confident that our \$31,743 portfolio will not lose more than \$267 on any given day. The Expected Shortfall is \$375. This is our average loss on the 5% of our worse days beyond the VAR threshold.

If we have \$250M to invest instead of \$31,743, our VaR would be `r VaR.hist * 250e6 / sum(weights.rf[1,])`, and our expected shortfall would be `r ES.hist * 250e6 / sum(weights.rf[1,])`. We note that the ES is `r 100 * 375.52/31743.64`%. This is less then if we held 100% of any of our commodities, as seen in the ES calculated individually for each.   

To avoid risk to or $250M, we want to be on the lefthand side of the distribution. To mitigate the risk of the scenarios on the righthand side, we want to hedge our portfolio for optimization.

### VaR Threshold Analysis 

####

The next step in our analysis of commodity market risk was to analyze the statistical attributes of each 0.01 quantile of the distribution of losses.  

We iterated through our loss data and for each 0.01 quantile, the data above the quantile was subsetted and its mean, standard deviation, number of data points, and pair of confidence intervals was calculated. This is assuming the each subset is normally distributed which presents its own risk in reliance.  

We then created a data frame of the 100 quantiles with their standard deviations and thresholds. The mean of the difference between each data point beyond the threshold and the threshold was calculated and labeled as the threshold exceedance. It is important to note this is not the same as the Expected Shortfall. Expected shortfall is the mean of the points beyond the threshold, whereas the threshold exceedance subtracts the threshold before taking the mean.

####

```{r, echo=FALSE, message=FALSE, warning=FALSE}

# Mean Excess Plot to determine thresholds for extreme event management

## Loss.rf was our data set of portfolio gains and losses

data <- as.vector(loss.rf) # data is purely numeric

# Identify the min loss and max profit, as the bounds that we will iterate between

umin <- min(data) 
umax <- max(data) - 0.1 # subtract a tad as we don't actually want to hit the higher bound

nint <- 100 # grid length to generate mean excess plot
grid.0 <- numeric(nint) # grid store

e <- grid.0 # store mean exceedances e
upper <- grid.0 # store upper confidence interval
lower <- grid.0 # store lower confidence interval

# Create an array that represents the losses associated with each percential

u <- seq(umin, umax, length = nint) # threshold u grid
alpha <- 0.95 # confidence level

# Iterate through 100 steps

for (i in 1:nint){
  data <- data[data > u[i]] # subset data above thresholds
  e[i] <- mean(data - u[i]) # calculate mean excess of threshold
  sdev <- sqrt(var(data)) # standard deviation
  n <- length(data) # sample size of subsetted data above thresholds
  upper[i] <- e[i] + (qnorm((1 + alpha)/2) * sdev)/sqrt(n) # upper confidence interval
  lower[i] <- e[i] - (qnorm((1 + alpha)/2) * sdev)/sqrt(n) # lower confidence interval
}

mep.df <- data.frame(threshold = u, threshold.exceedances = e, lower = lower, upper = upper) 

loss.excess <- loss.rf[loss.rf > u]

# Plot the mean loss beyond the threshold, against the threshold

p2 <- ggplot(mep.df, 
            aes(x = threshold,
                y = threshold.exceedances)) + 
  theme_light() +
  geom_line() +
  geom_line(aes(x = threshold, y = lower),
            colour = "red") + 
  geom_line(aes(x = threshold,y = upper), 
            colour = "red") + 
  annotate("text",x = 400, y = 200, label = "upper 95%") +
  annotate("text", x = 200, y = 0,label = "lower 5%")+ labs(x= "Threshold", y="Threshold Exceedance") +
  theme(legend.position='none')

ggplotly(p2)

```

####

We observe that as the threhold increases, our mean loss initially falls. When the threshold exceeds \$100 the corresponding mean becomes eractic because of the small number of data points included in the mean. The mean then increases until a loss threshold of approximately \$350. This corresponds to the 0.87 quantile of the distribution of gains and losses. We note that by the 0.95 quantile, the mean of the losses beyond the threshold has fallen considerably.

If our approach is risk averse, we may wish to use a threshold of 0.87 instead of 0.95, as it indicates slightly higher losses compared to the threshold.


Optimize Risky Portfolio    {.tabset}   
=====================================

Chart column
---------------------------

### Optimal 4-Security Portfolio    

```{r, echo=FALSE}

assets <- 4 # number of assets

R <- asset_returns[, 3:6]/100

names.R <- colnames(R)
mean.R <- apply(R, 2, mean)
cov.R <- cov(R)
sd.R <- sqrt(diag(cov.R)) ## remember these are in daily percentages
# library(quadprog)
Amat <- cbind(rep(1, assets), mean.R) ## set the equality constraints matrix

n <- 300 # number of portfolios to generate

## set of 300 possible target portfolio returns
mu.P <- seq(0.5 * min(mean.R), 1.5 * max(mean.R), length = n) 

# possible target portfolio returns
sigma.P <- mu.P ## set up storage for std dev's of portfolio returns
weights <- matrix(0, 
                  nrow = n, 
                  ncol = ncol(R)) ## storage for portfolio weights

colnames(weights) <- names.R

for (i in 1:length(mu.P)) {
  bvec <- c(1, mu.P[i]) ## constraint vector
  result <- solve.QP(Dmat = 2 * cov.R,
                     dvec = rep(0, assets), 
                     Amat = Amat,
                     bvec = bvec, 
                     meq = 2)
  sigma.P[i] <- sqrt(result$value)
  weights[i, ] <- result$solution
  }

sigma.mu.df <- data.frame(sigma.P = sigma.P,mu.P = mu.P)

# set risk free rate - business prob said to use USD TBill for 2/28/2018
mu.free <- asset_returns[asset_returns$Date == "2018-02-28" , 7]  / 100

sharpe <- (mu.P - mu.free)/sigma.P ## compute Sharpe's ratios
ind <- (sharpe == max(sharpe)) ## Find maximum Sharpe's ratio
ind2 <- (sigma.P == min(sigma.P)) ## find the minimum variance portfolio
ind3 <- (mu.P > mu.P[ind2]) ## finally the efficient frontier
col.P <- ifelse(mu.P > mu.P[ind2], "blue", "grey")
sigma.mu.df$col.P <- col.P

```


```{r}
# renderPlotly({
p <- ggplot(sigma.mu.df, 
            aes(x = sigma.P, y = mu.P, group = 1)) + 
  geom_line(aes(colour = col.P, group = col.P)) + 
  scale_colour_identity() +
# + xlim(0, max(sd.R*1.1)) + ylim(0, max(mean.R)*1.1) +
  geom_point(aes(x = 0, y = mu.free), colour = "red") +
#options(digits = 4)+ 
  geom_point(aes(x = sigma.P[ind], y = mu.P[ind]), color='green') +
  geom_abline(intercept = mu.free,
                 slope = (mu.P[ind] - mu.free)/sigma.P[ind], 
                 colour = "red") + 
  geom_point(aes(x = sigma.P[ind2],
                 y = mu.P[ind2])) + ## show min var portfolio

  geom_point(aes(x = sd.R[1],
                 y = mean.R[1]),
             color='thistle') + ## show min var portfolio
  geom_point(aes(x = sd.R[2],
                 y = mean.R[2]),
             color='lightblue') + ## show min var portfolio
  geom_point(aes(x = sd.R[3],
                 y = mean.R[3]),
             color='orange') + ## show min var portfolio
  geom_point(aes(x = sd.R[4],
                 y = mean.R[4]),
             color='darkgrey') + ## show min var portfolio

  #  labels
    annotate("text", 
           x = sigma.P[ind] - 0.005, 
           y = mu.P[ind],
           label = "Optimal",
           color='green') +
    annotate("text", 
           x = sd.R[1] + 0.005,
           y = mean.R[1], 
           label = names.R[1],
           color='thistle') +
    annotate("text", 
           x = sd.R[2] + 0.006, 
           y = mean.R[2] + 0.0001, 
           label = names.R[2],
           color='lightblue') + 
    annotate("text", 
           x = sd.R[3] + 0.005, 
           y = mean.R[3] + 0.0001, 
           label = names.R[3],
           color='orange') + 
  annotate("text", 
           x = sd.R[4] + 0.006, 
           y = mean.R[4] - 0.0001, 
           label = names.R[4],
           color='darkgrey') + 
  annotate("text", 
           x = 0.002 , 
           y = 0.001, 
           label = "US T-Bill \n (2/28/18)",
           color='red') +
  theme_light() +
  theme(legend.position='none') # remove ugly plotly legend

ggplotly(p)

```

### Description of Weights

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# print out the results...
output <- paste("The optimal mix is ZAR ", 
      format(6e3 * weights[ind,1],big.mark=",", scientific=FALSE), 
      "M of LITECOIN, and ZAR ",
      format(6e3 * weights[ind,2],big.mark=","),
      "M of Diamond, nand ZAR ",
      format(6e3 * weights[ind,3],big.mark=","),
      "M of SILVER, and ZAR ",
      format(6e3 * weights[ind,4],big.mark=",", scientific=FALSE),
      "M of Platinum.  This mix has a standard deviation of ZAR ",
      format(6e3 * sigma.mu.df[ind,1],big.mark=",", scientific=FALSE),
      "M and an expected daily return of ZAR ",
      format(6e3 * sigma.mu.df[ind,2],big.mark=",", scientific=FALSE),
      "M",
      sep="")

cat(output)
```




Weights column      {data-width=300}
------------------------------------

### Results    {data-height=300} 

We generate a range of possible returns, ranging from half the return of the lowest performing asset to 1.5 times the highest performing asset.  For each return, we solve a quadratic equation to identify the lowest risk with any mix of assets that will provide that return.  These returns and risks are plotted as the blue line on the above plot, which is known as the efficient frontier.

So which return should we use?  What is the optimal mix of return and risk?  We plot a red point indicating the risk free return - the return on a security that is short-term and safe, which represents a return that can be achieved without risk.  We then plot a line from our risk free return to where it tangents the efficient frontier.  The slope of this line is the Sharpe Ratio.  The point where it tangents the efficient frontier is the optimal portfolio.

Because some of our securities have low or negative returns, our efficient frontier is low, and the tangent is therefore correspondingly high.  This in turn creates a portfolio where we leverage returns by shorting two of the securiites with negative returns (Platinum and BERKSHIRE).

De-risk   
=====================================

Column: Pick a Risk ... any Risk    {data-width=150}  
--------------------------------------------------------------------------

### Target Daily Return   {data-height=100}  

```{r}

### Shiny Reactive Source
sliderInput("mu.Target", 
            label = "Target Annual Return (%)", 
            min= round(mu.free * 36500, 2), # lower limit is risk free rate 
            max = round(mu.P[ind] * 36500, 2) , # upper limit is return of tangent in plot 
            value = round(mu.P[ind] * 36500, 2) ,  # initial value is upper end of scale
            step = 0.1)

```


### Target Annual Return  

```{r}
renderValueBox({
  # get return from slider
  mu.Target.rx <- reactive(input$mu.Target)
  
  payload <- paste( as.character(mu.Target.rx()), "%", sep="")
  valueBox(payload, 
           icon = "fa-bars", 
           color="burlywood") 
})
```

### Target Daily Return  

```{r}
renderValueBox({
  # get return from slider
  mu.Target.rx <- reactive(input$mu.Target / 36500.0)
  
  payload <- paste( as.character(100.0 * round(mu.Target.rx(),5)), "%", sep="")
  valueBox(payload, 
           icon = "fa-bars", 
           color="sandybrown") 
})
```


### Daily Variance  

```{r}
renderValueBox({
  # get return from slider
  mu.Target.rx <- reactive(as.numeric(input$mu.Target)/36500.0)

  # calc sigma by dividing return by slope 
  sharpeline.sigma.rx <-   reactive( (mu.Target.rx() -  mu.free) / sharpe.slope )
  
  payload <- as.character(round(sharpeline.sigma.rx() ,5))
  valueBox(payload, 
           icon = "fa-bars", 
           color="SILVERenrod") 
})
```



Column: Same Return for Reduced Risk   
--------------------------------------------------------------------------

### 5-Security Portfolio    


```{r}
      assets = 5

      # create a vector of perpetual risk free returns
      mu.free <- asset_returns[asset_returns$Date == "2018-02-28" , 7]  / 100.0
      # 0.004465753
      
      # create a portfolio of hist returns of 5 securities
      R5 <- asset_returns[,3:7] / 100

      # set up all the variables needed to solve for max(risk)
      mean.R <- apply(R5, 2, mean)
      cov.R <- cov(R5)

      Dmat5 <- 2 * cov.R
      Amat5 <- cbind(rep(1, assets), mean.R) ## set the equality constraints matrix
      dvec5 <- rep(0, assets)
      
      sharpe.slope <- (mu.P[ind] - mu.free) / (sigma.P[ind] - 0.0)
      # 0.06549647
      
```


```{r}
## DEBUG - let's start simple with just a non-reactive ggplot()

renderPlotly({
    mu.Target.rx <- reactive(as.numeric(input$mu.Target)/36500.0)

    # calc sigma by dividing return by slope 
    sharpeline.sigma.rx <-   reactive( (mu.Target.rx() -  mu.free) / sharpe.slope )
    
      p <- ggplot(sigma.mu.df, aes(x = sigma.P, y = mu.P, group = 1)) + 
            geom_line(aes(colour = col.P, group = col.P)) + # blue/grey elipse
            geom_point(aes(x = 0, y = mu.free), colour = "red") + # red dot for min return
            geom_point(aes(x = sigma.P[ind], y = mu.P[ind]), color='green') + # green dot for max return 
            geom_point(aes(x = sharpeline.sigma.rx(), y = mu.Target.rx() ), color = 'SILVER') + # moving dot
            geom_hline(yintercept = mu.Target.rx(), color = 'SILVER' ) +  # moving hline
            geom_vline(xintercept = sharpeline.sigma.rx() , color = 'SILVER') +
            geom_abline(intercept = mu.free,
                           slope = (mu.P[ind] - mu.free)/sigma.P[ind], 
                           colour = "red") + # red Sharpe line
            scale_colour_identity() +
            theme(legend.position='none') + # remove ugly plotly legend 
            theme_light() 

      ggplotly(p)
})
```


### Explanation      {data-height=150}  

We mentioned that the second function of the Sharpe Ratio line is to create a new efficient frontier that represents the optimal portfolio when we add a risk-free security into the portfolio.  Adding a risk-free security reduces the average risk of the portfolio, which causes the optimization mix of the other four assets to change.  With the risk-free security included in the portfolio, we get more return for a given level of risk because the portfolio is on the red Sharpe line and not the blue frontier of the 4-security portfolio.  But on a straight line there is no optimal return - the business must choose either its target return or its target risk.  The slider here allows you to select a target annual return, the optimization process fires off, and the corresponding risk and portfolio weights are displayed.

Column: Portfolio Weights    {data-width=150}  
--------------------------------------------------------------------------

#### Asset Weights

### LITECOIN   

```{r}

renderValueBox({
      mu.Target.rx <- reactive(as.numeric(input$mu.Target)/36500)
  
      bvec.rx <- reactive(c(1, mu.Target.rx())) ## constraint vector

      # solve 5 security quadratic for slider return
      result.rx <- reactive(
                          solve.QP( Dmat = Dmat5,
                                    dvec = dvec5, 
                                    Amat = Amat5,
                                    bvec = bvec.rx(), 
                                    meq = 2)
      )

    # results of quadratic solution
    weights.rx <- reactive(result.rx()$solution)
   
  payload <- paste(as.character(round(100.0 * weights.rx()[1], 1)) , "%", sep="")
  valueBox(payload, icon = "fa-ship", color="thistle")

  })

```

### BERKSHIRE   

```{r}

renderValueBox({

      mu.Target.rx <- reactive(as.numeric(input$mu.Target)/36500)
  
      bvec.rx <- reactive(c(1, mu.Target.rx())) ## constraint vector

      # solve 5 security quadratic for slider return
      result.rx <- reactive(
                          solve.QP( Dmat = Dmat5,
                                    dvec = dvec5, 
                                    Amat = Amat5,
                                    bvec = bvec.rx(), 
                                    meq = 2)
      )

    # results of quadratic solution
    weights.rx <- reactive(result.rx()$solution)
   
  payload <- paste(as.character(round(100.0 * weights.rx()[2], 1)) , "%", sep="")
  valueBox(payload, icon = "fa-ship", color="lightblue")

  })

```

### SILVER   

```{r}

renderValueBox({

      mu.Target.rx <- reactive(as.numeric(input$mu.Target)/36500)
  
      bvec.rx <- reactive(c(1, mu.Target.rx())) ## constraint vector

      # solve 5 security quadratic for slider return
      result.rx <- reactive(
                          solve.QP( Dmat = Dmat5,
                                    dvec = dvec5, 
                                    Amat = Amat5,
                                    bvec = bvec.rx(), 
                                    meq = 2)
      )

    # results of quadratic solution
    weights.rx <- reactive(result.rx()$solution)
   
  payload <- paste(as.character(round(100.0 * weights.rx()[3], 1)) , "%", sep="")
  valueBox(payload, icon = "fa-ship", color="orange")

  })

```

### Platinum   

```{r}

renderValueBox({

      mu.Target.rx <- reactive(as.numeric(input$mu.Target)/36500)
  
      bvec.rx <- reactive(c(1, mu.Target.rx())) ## constraint vector

      # solve 5 security quadratic for slider return
      result.rx <- reactive(
                          solve.QP( Dmat = Dmat5,
                                    dvec = dvec5, 
                                    Amat = Amat5,
                                    bvec = bvec.rx(), 
                                    meq = 2)
      )

    # results of quadratic solution
    weights.rx <- reactive(result.rx()$solution)
   
  payload <- paste(as.character(round(100.0 * weights.rx()[4], 1)) , "%", sep="")
  valueBox(payload, icon = "fa-ship", color="lightgrey")

  })

```

### T-Bill   

```{r}

renderValueBox({

    mu.Target.rx <- reactive(as.numeric(input$mu.Target)/36500)

    bvec.rx <- reactive(c(1, mu.Target.rx())) ## constraint vector

    # solve 5 security quadratic for slider return
    result.rx <- reactive(
                        solve.QP( Dmat = Dmat5,
                                  dvec = dvec5, 
                                  Amat = Amat5,
                                  bvec = bvec.rx(), 
                                  meq = 2)
    )

  # results of quadratic solution
  weights.rx <- reactive(result.rx()$solution)
   
  payload <- paste(as.character(round(100.0 * weights.rx()[5], 1)) , "%", sep="")
  valueBox(payload, icon = "fa-ship", color="lightgreen")

  })

```


Findings     {.tabset}  
====================================

Column: Answer the Questions   
--------------------------------------------------------------------------

#### Business Questions   

_Are there signs in the historical returns of the securities that indicate that the portfolio lacks diversification?_ 


_What losses might we expect from the portfolio on the worst 5% of days?_   


_If we model the portfolio's losses by fitting a Pareto distribution, what losses do we observe from the fitted distribution?_  


_With just the four securities, each susceptible to market risk, what is the optimal mix for our ZAR 6B portfolio, and what return will this provide?_  


_If we were to introduce a risk-free security into the portfolio to reduce the volatility risk, how much does the risk reduce for a range of target returns?_  


Column: Variance with Other Assets   
-------------------------------------------------------------------------- 

The CFO also asked the team do some sensitivity analysis on the portfolio.  Team members should switch out one of the securities and adjust the analysis as follows:  
* Replace LITECOIN with another crypto currency (Nick)
* Replace GOLD with silver (Jason)
* Replace platinum with palladium (Karen)



Column: Conclusions   
-------------------------------------------------------------------------- 

### Conclusion  

We conclude that ...

